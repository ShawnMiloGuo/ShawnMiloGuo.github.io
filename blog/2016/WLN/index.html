<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 对抗样本噪声的遥感深度学习分类网络模型 | 郭善昕 Shanxin Guo </title> <meta name="author" content="郭善昕 Shanxin Guo"> <meta name="description" content="蒸散空间下的土壤特征空间构建"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shawnmiloguo.github.io/blog/2016/WLN/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">郭善昕</span> Shanxin Guo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">主页 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">博客 </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">论文 </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">项目 </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">团队 </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">新闻 </a> </li> <li class="nav-item "> <a class="nav-link" href="/recruitment/">招聘 </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div style="display: none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\bsa}{\boldsymbol{a}} \newcommand{\bsb}{\boldsymbol{b}} \newcommand{\bsc}{\boldsymbol{c}} \newcommand{\bsd}{\boldsymbol{d}} \newcommand{\bse}{\boldsymbol{e}} \newcommand{\bsoldf}{\boldsymbol{f}} \newcommand{\bsg}{\boldsymbol{g}} \newcommand{\bsh}{\boldsymbol{h}} \newcommand{\bsi}{\boldsymbol{i}} \newcommand{\bsj}{\boldsymbol{j}} \newcommand{\bsk}{\boldsymbol{k}} \newcommand{\bsell}{\boldsymbol{\ell}} \newcommand{\bsm}{\boldsymbol{m}} \newcommand{\bsn}{\boldsymbol{n}} \newcommand{\bso}{\boldsymbol{o}} \newcommand{\bsp}{\boldsymbol{p}} \newcommand{\bsq}{\boldsymbol{q}} \newcommand{\bsr}{\boldsymbol{r}} \newcommand{\bss}{\boldsymbol{s}} \newcommand{\bst}{\boldsymbol{t}} \newcommand{\bsu}{\boldsymbol{u}} \newcommand{\bsv}{\boldsymbol{v}} \newcommand{\bsw}{\boldsymbol{w}} \newcommand{\bsx}{\boldsymbol{x}} \newcommand{\bsy}{\boldsymbol{y}} \newcommand{\bsz}{\boldsymbol{z}} \newcommand{\bsA}{\boldsymbol{A}} \newcommand{\bsB}{\boldsymbol{B}} \newcommand{\bsC}{\boldsymbol{C}} \newcommand{\bsD}{\boldsymbol{D}} \newcommand{\bsE}{\boldsymbol{E}} \newcommand{\bsF}{\boldsymbol{F}} \newcommand{\bsG}{\boldsymbol{G}} \newcommand{\bsH}{\boldsymbol{H}} \newcommand{\bsI}{\boldsymbol{I}} \newcommand{\bsJ}{\boldsymbol{J}} \newcommand{\bsK}{\boldsymbol{K}} \newcommand{\bsL}{\boldsymbol{L}} \newcommand{\bsM}{\boldsymbol{M}} \newcommand{\bsN}{\boldsymbol{N}} \newcommand{\bsP}{\boldsymbol{P}} \newcommand{\bsQ}{\boldsymbol{Q}} \newcommand{\bsR}{\boldsymbol{R}} \newcommand{\bsS}{\boldsymbol{S}} \newcommand{\bsT}{\boldsymbol{T}} \newcommand{\bsU}{\boldsymbol{U}} \newcommand{\bsV}{\boldsymbol{V}} \newcommand{\bsW}{\boldsymbol{W}} \newcommand{\bsX}{\boldsymbol{X}} \newcommand{\bsY}{\boldsymbol{Y}} \newcommand{\bsZ}{\boldsymbol{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\mathbbm}{\Bbb} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <header class="post-header"> <h1 class="post-title">对抗样本噪声的遥感深度学习分类网络模型</h1> <p class="post-meta"> Created in February 03, 2016 </p> <p class="post-tags"> <a href="/blog/2016"> <i class="fa-solid fa-calendar fa-sm"></i> 2016 </a>   ·   <a href="/blog/tag/%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E6%9E%84%E5%BB%BA"> <i class="fa-solid fa-hashtag fa-sm"></i> 特征空间构建</a>   ·   <a href="/blog/category/%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E4%BC%98%E5%8C%96"> <i class="fa-solid fa-tag fa-sm"></i> 特征空间优化</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>针对样本标签噪声造成的地物提取精度不足问题，本文提出了一种抗噪声标签的卷积神经网络框架，Weight Loss Net（WLN）。WLN主要包含三部分：（1）分割子网络，用于产生图像的逐像素分类结果，可以使用其他的分割模型进行替换；（2）损失权重参数，用于对每个训练样本赋权重，对干净样本赋予高权重值，对噪声样本赋予低权重值，降低噪声样本对网络训练过程中的影响，提高网络的抗噪性能；（3）类别平衡系数，帮助网络平等地学习每一个类别，避免由于不同类别之间的不平衡导致模型过拟合。 基于上述方法在公开数据集（Inria Aerial Image Labeling Dataset,以下简称Inria）上实验进行建筑物提取，通过图像膨胀和腐蚀操作模拟四种标签噪声类型（多标注噪声，少标注噪声，错标注噪声和漏标注噪声）。本研究在训练数据集加入不同噪声率及噪声等级的训练样本，并在干净的数据集进行测试，与原U-Net网络进行比较，评估网络的抗噪性能。</p> <p>实验结果表明，当噪声增加的时候，WLN能够保持模型的性能和维持较高的精度，而U-Net模型的精度则有所下降。在噪声率及噪声等级比较低的时候，U-Net并不会受到噪声标签的影响，这是因为深度学习模型具有一定的抗噪性；当噪声率及噪声等级逐渐增加时，U-Net网络精度下降明显。在噪声不断增加的时候，我们提出的WLN可以一直保持很高的精度。这一结果表明，本文提出的抗噪声标签的卷积网络框架可以降低噪声训练标签对分隔模型的影响。</p> <h1 id="原文链接">原文链接</h1> <p>Lin, C., Guo, S., Chen, J., Sun, L., Zheng, X., Yang, Y., &amp; Xiong, Y. (2021). Deep Learning Network Intensification for Preventing Noisy-Labeled Samples for Remote Sensing Classification. Remote Sensing, 13(1689), 1–19.</p> <h1 id="研究背景">研究背景</h1> <p>深度学习方法性能好坏取决于两个因素，训练数据量的大小以及标签标注的准确性。相比于数据量小的问题，标签标注噪声问题更难解决。遥感数据集中标签噪声普遍存在。一是因为遥感图像土地类型复杂，需要有一定专业知识才能够对其进行准确标注 。二是因为多个专家同时对同一副遥感图像进行标注，每个专家之间的标注结果不一致 。三是低成本的自动化标注或者缺乏专业知识的人员进行标注，标注的结果往往可靠性较差。标签噪声通过影响网络的损失值，使得网络参数往错误的方向更新，降低了网络的分类性能。因此深度学习中的标签噪声问题处理方法主要分为两类，一类是针对标签，一类是针对损失值。针对标签进行处理的方法有两种实现形式，一种是对训练数据进行筛选，一种是计算转移矩阵。样本筛选，这类方法的主要思想是从一个有噪声的训练数据集中选择干净的样本进行网络训练。最初，MALACH E等人认为噪声样本会对网络进行错误的更新，即使网络能够预测到正确的样本，也会由于噪声标签使得此时调整好的网络往错误的方向更新，因此提出将何时更新与怎么更新进行分离，使用两个卷积神经网络，只有当两个网络预测结果不一致的时候才进行参数更新。之后，ARPIT D等人发现在网络训练过程中，网络倾向于先学习干净的样本，再慢慢拟合噪声样本及困难样本。因此，许多研究采用小损失选择准则，将一定数量的小损失训练样本作为干净样本。HUANG J等人就是基于这一准则，通过多次的循环从欠拟合到过拟合的过程，根据样本的loss曲线，把loss高的样本作为噪声样本剔除，保留小损失样本 。JIANG L等人介绍了一种协助学习的模式，网络分为两个部分，教师网络和学生网络，基于小损失样本选择准则，教师网络向学生网络筛选出正确的样本，学生网络根据教师网络提供的样本进行网络训练。HAN B等人认为一个网络对于标签噪声的学习中错误会不断累加，多个网络具有不同的学习能力，可以过滤不同类型的噪声标签带来的错误，因此采用了两个CNN网络，每个CNN网络都选择一定数量的小损失样本，并将其反馈给另一个网络进行训练。这类方法可以通过简单地排除不可靠的样本，有效地避免噪声标签对网络的影响。但是这类方法的关键在于能否设计有效的样本选择策略，如果选择策略不好，可能排除大量有用的样本。</p> <p>第二种实现形式是计算噪声转移矩阵，该矩阵定义了一个类别变换到另一个类别的概率，通过计算噪声转移矩阵，将噪声标签纠正为干净标签。CHEN X等人在网络末尾添加一层线性层作为噪声适应层，估计标签标注类别被翻转为其他类的概率，之后推断出真实的标签，进行网络参数更新。XIAO T等人提出的网络分为三个部分，分类子网络和噪声子网络以及噪声适应层，分类子网络可以得到分类的概率，噪声子网络可以得到标签为某一类噪声的概率，噪声适应层将两个子网络的输出作为输入推断出真实标签的概率。网络分为两阶段训练，首先采用干净的样本对分类子网络和噪声子网络做预训练，之后再用带噪声标签的数据集进行训练，推断出真实的标签，更新网络模型，提高网络性能。SUKHBAATAR S等人同样采用两阶段训练，首先使用简单易分辨的图像对网络进行预训练，之后再使用含噪声标签的数据集对网络最后一层噪声适应层进行微调。这类方法很大程度上依赖对于噪声分布的准确假设，而在实际中，这种分布很难准确估计出来。</p> <p>第二类方法主要是针对损失值进行处理，有两种实现形式，一种是修改网络的损失函数，一种是对损失值进行赋权重。修改网络损失函数的方法基本思路是设计一个鲁棒性的损失函数，即使训练数据中含有噪声标签，也可以使得计算出来的损失值不受噪声数据的影响。MANWANI N等人提出对于二值分类情况下，神经网络的损失函数对标签噪声有天然的鲁棒性。GHOSH A等人对这一理论推导至多分类的情况，并且对比了MAE,MSE,CCE损失函数对标签噪声的鲁棒性，证明了MAE损失函数对噪声具有更强的鲁棒性。ZHANG Z等人在前者的基础上发现MAE在复杂数据集中对噪声标签的鲁棒性表现不佳，文章对MAE进行修改，结合CCE，提出GCE损失函数。受KL散度对称性的启发，WANG Y等人提出对称性的交叉熵损失函数，用对噪声鲁棒的反交叉熵损失函数对交叉熵损失函数进行增强，解决交叉熵损失函数在有噪声标签数据集中的学习不足和过拟合现象。设计鲁棒性的损失函数虽然可以提高网络的抗噪性能，但是网络仍然会受到标签噪声的影响，并且只在简单的情况下执行得比较好，例如容易分别的类或者类别数量少的情况，同时损失函数的修改会增加训练收敛的时间。</p> <p>第二种实现形式是对损失值赋权重，基本思路是对所有训练样本损失值分配权重，并在训练过程中迭代更新这些权重。LIU 等人证明了通过对训练样本进行重要性加权，可以使得标签噪声训练下的分类器达到无噪声分类器的性能。REN M等人使用元学习范式基于梯度方向调整训练样本的损失权重。首先获取一个干净标签的数据集作为验证集，每一轮网络训练后在验证集上进行验证，计算验证集的损失值，之后由验证集的损失值得到训练样本的权重并对网络进行参数更新。SHU J等人也采用了类似的方法，但不是隐式计算权重，而是使用多层感知机估计权重值。XUE C等人不但通过网络迭代区分干净样本和噪声样本，同时根据噪声水平对样本进行加权提取出噪声样本和困难样本中的有用信息。这类方法的有效性主要取决于样本权重调整方案是否能够正确地提高干净标签样本的损失权重值，降低噪声标签样本的损失权重值。此外，如今的标签噪声处理方法大部分是针对于场景识别任务，对于遥感图像分类中标签噪声的处理研究很少，上述方法只适用于场景识别任务，不适用于遥感图像分类任务。</p> <p>在本文中，我们提出了一种适用于遥感图像分类中标签噪声处理的通用网络框架，Weight LossNet（WLN）。WLN对标签噪声的处理思路主要采用了上面提到的对损失值赋权重。其中对样本权重的调整方案结合注意力机制，通过注意力机制网络得出每个样本的重要性，提高重要样本的损失权重值，降低非重要样本的损失权重值，避免标签噪声对模型的影响。之后在遥感图像公开数据集Inria Aerial Image Labeling Dataset上进行实验，并与原始的分类网络方法进行对比。本文的主要贡献总结如下：（1）研究卷积神经网络对于四种常见的标签噪声类型(不足标签、冗余标签、缺失标签、错误标签)的鲁棒性。（2）提出一种适用于遥感图像分类中标签噪声处理的通用网络框架，该算法在不同噪声类型水平下都能保持较高的精度和较好的泛化性能。</p> <h1 id="数据源及覆盖区域">数据源及覆盖区域</h1> <p>在本文的研究中，采用Inria Aerial Image Labeling Dataset（以下简称为Inria数据集）。该数据集解决了遥感领域最重要的问题之一：航空图像的像素级自动标注。Inria数据集图像分辨率为30cm,标签标注两类信息，建筑类别和非建筑类别。同时这些图像覆盖了不同的城市地区，不但有建筑物稠密的大城市还有建筑物稀少的小镇。</p> <p>Inria训练数据集包含180副5000*5000大小的图像，覆盖Austin, Chicago, Kitsap County, Western Tyrol, and Vienna这5个地区，总面积为405平方公里。标签由180张单通道的图像组成，其中255表示建筑类别，0表示非建筑类别，具体图像如图3.5.1所示。由于此数据集是用于比赛，无法获取到测试集的标签，因此本研究中将原训练集按照8:1:1的比例分为训练集，验证集和测试集三部分，三者相互独立，互不重叠。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320180501-00sj5ku.png" alt="image.png" class="rounded"></p> <p><a href="">图</a>3.5.1从左到右分别为Austin，Chicago，Kitsap County，Western Tyrol和Vienna的图像及对应的标签图</p> <h1 id="实验噪声设置">实验噪声设置</h1> <p>标签噪声定义为实例与标注的所属类别不相对应，即标注错误。在像素级遥感土地覆被标注中，有四种常见的标签噪声，1）多标注噪声，即标签面积大于实际面积；2）少标注噪声，即标签面积小于实际面积；3）错标注噪声，即将对象标注为错误的类别，4）漏标注噪声，即对象的整个标签缺失。</p> <p>为了模拟标签噪声，我们采用不同的卷积核对标签进行图像膨胀和腐蚀操作。如图3.5.2所示，第一行表示图像膨胀处理，模拟多标注噪声（如图3.5.2的c，d所示，分别对实际标签进行卷积核为9<em>9，17</em>17的图像膨胀操作）。当卷积核大小增加到25*25时，标签中包含很多错误的像素，可以认为是错标注噪声样本（如e所示）。同理，采用图像腐蚀操作模拟少标注噪声（如图3.5.2的f,g）和漏标注标签样本（h）。为了明确多标注和少标注噪声不同的误差，在本研究中，我们设置了三个噪声等级，我们将卷积核为9的噪声表示Noise level 1，卷积核为17表示Noise level 2，卷积核为25表示Noise level 3。本研究设置了5个数据集样本噪声率，以测试网络在不同错误样本数量下的表现。我们从训练集中随机选取0%、25%、35%、45%和50%的噪声样本对训练数据集进行噪声处理。具体噪声标签图像如图3.5.2所示。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320180701-kbf1dgb.png" alt="image.png" class="rounded"></p> <p><a href="">图</a>3.5.2四种噪声标签的图像</p> <p>图3.5.2第一列从左到右分别是原图，干净标签图，Noise level1的多标注噪声标签图，Noise level 2的多标注噪声标签图，Noise level 3的错标注噪声标签图；第二列从左到右分别为原图，干净标签图，Noiselevel 1的少标注噪声标签图，Noise level 2的少标注噪声标签图，Noise level 3的漏标注噪声标签图。</p> <h1 id="wln-网络结构">WLN 网络结构</h1> <p><img src="/SIAT-GeoScience/assets/image-20220320180814-c8vmves.png" alt="image.png" class="rounded"></p> <p><a href="">图</a>3.5.3 WLN的网络结构图</p> <p>WLN网络包含两个子网络，分割子网络和注意力子网络，其中分割子网络由分割模型组成，用于产生分割结果并与标签计算得到损失值，注意力子网络由CNN模型和注意力模块组成，用于计算训练样本的损失权重值。为了解决当腐蚀噪声中标签类别严重失衡而引起注意力子网络无法对训练样本赋予正确损失权重的问题，我们通过标签计算类别平衡系数进行类别平衡，最后将损失值，损失权重值，类别平衡系数三者结合，作为最终的损失值，对分割子网络和注意力子网络进行反向传播更新参数。</p> <p>WLN网络的分割子网络采用的是U-Net分割模型，也可以替换成其他的分割模型。注意力机制子网络实现形式采用的是SE模块，SE模块是胡杰及其团队于2017年提出的一种卷积神经网络结构，如图3.5.4所示。SE模块的提出是为了考虑特征图中通道之间的关系，并为每个通道提供不同的权重。它通过学习自动获取每个特征通道的重要性，然后根据每个通道的重要性加强有用的特征，而抑制对当前任务无用的特征。squeeze和excitation是SE模块中的两个关键操作。这两个操作可以帮助SE模型捕获通道侧的依赖性，并大大减少参数和计算的数量。</p> <h1 id="实验结果及分析">实验结果及分析</h1> <p>本节按照上一章介绍的参数设置进行模型训练得到对应的WLN模型和U-Net模型，在相同的测试集中对两个模型进行评价，本文从定量提取精度和定性地物提取细节两个角度，按照上节所述的评价指标对地物提取结果进行评价。同时，在讨论部分对引进的两个参数，损失权重值和类别平衡系数的有效性进行分析。</p> <p>下面我们分别比较WLN和U-Net对于腐蚀和膨胀噪声类型的测试集提取精度结果。</p> <p>（1）膨胀噪声</p> <p>我们分别使用不同噪声率及不同噪声等级的膨胀噪声标签训练集对WLN网络和U-Net网络进行训练，并在干净的标签测试集上进行测试。表3.5.1和图3.5.8显示了两种方法在测试集的提取精度结果。从精度曲线图上可以看出，在干净的数据集中，U-Net和WLN都可以保持很高的精度。随着噪声率的不断增加，网络精度并不是立马下降。在Noise level 1下， U-Net方法仍能保持较高的精度，这可能是因为卷积神经网络具有特定的抗噪声能力。但是，当噪声率在50%时，U-Net的精度明显下降，在Noise level 2时，OA下降4.4%，MIOU下降7.7%，Kappa下降2.0%；在Noise level 3时，OA下降12.7%，MIOU下降20.7%，Kappa下降13.8%。相比之下，WLN在噪声率增加的情况下仍能保持较高的精度，噪声率在50%时，在Noise level 2时，OA下降1.2%，MIOU下降2.1%，Kappa下降1.1%；在Noise level 3时，OA下降0.2%，MIOU下降0.3%，Kappa下降0.8%。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320181106-xhrl4z3.png" alt="image.png" class="rounded"></p> <p>（2）腐蚀噪声</p> <p>我们分别使用不同噪声率及不同噪声等级的腐蚀噪声标签训练集对WLN网络和U-Net网络进行训练，并在干净的标签测试集上进行测试。表3.5.2和图3.5.9显示了两种方法在测试集的提取精度结果。从精度曲线上可以看出，网络在腐蚀噪声中的精度变化与膨胀噪声中的精度变化相似。U-Net网络在噪声率以及噪声等级低的时候，由于自身的抗噪能力，可以避免噪声的影响。随着噪声率的增加（在Noiselevel 3，噪声率为50%时），U-Net方法在测试集上的OA精度在这个过程下降了8.4%，MIOU下降了24.2%，Kappa系数下降了43.3%，变化幅度较大。而WLN稳定性更好，OA在这个过程只变化了1.5%，MIOU变化了4.7%，Kappa系数变化了0.5%。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320181202-2xgkaeh.png" alt="image.png" class="rounded"></p> <h1 id="提取细节比较">提取细节比较</h1> <p>这一小节我们通过目视解译的方式来评估WLN和U-Net模型在膨胀和腐蚀标签噪声下地物提取细节。</p> <p>（1）膨胀噪声</p> <p>图3.5.10是膨胀噪声类型下在噪声率为50%不同噪声等级下两种方法的提取结果。第一行和第二行是Noiselevel 1提取结果，第三行和第四行是Noise level 2的提取结果，第五行和第六行是Noise level 3的提取结果。从图中可以看出，在噪声级别比较低的时候，如Noiselevel 1，WLN和U-Net的提取结果相似，但是U-Net由于受到膨胀噪声的影响会将一些非建筑物的像素点分类为建筑物，如图中红框所示。当噪声级别比较高的时候，如Noiselevel 3，U-Net受膨胀噪声影响比较大，对于建筑物只能提取出大概的轮廓，会将建筑物中的街道也标记为建筑物，而WLN方法对于噪声的影响较小，可以很好的识别建筑物及建筑物之间的界限。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320181325-5bh6lsj.png" alt="image.png" class="rounded"></p> <p><a href="">图</a>3.5.10膨胀噪声类型下U-Net和WLN的提取结果图</p> <p>（2）腐蚀噪声</p> <p>图3.5.11是腐蚀噪声类型下在噪声率为50%不同噪声级别两种方法的提取结果。第一行和第二行是Noise level 1提取结果，第三行和第四行是Noise level 2的提取结果，第五行和第六行是Noise<br> level 3的提取结果。从图中可以看出，在噪声级别比较低时，如Noise level 1，WLN和U-Net的提取结果相对较完整，但是U-Net由于受到腐蚀噪声的影响，会将一些建筑物像素点分类为非建筑物，如图中红框所示，随着噪声级别的增加，这种影响会越来越大，如在Noise level 3中，U-Net基本已经无法对建筑物进行分类，而WLN即使在高噪声级别下依旧可以得到很好的分类结果。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320181435-8ifzmqa.png" alt="image.png" class="rounded"></p> <p><a href="">图</a>3.5.11腐蚀噪声类型下U-Net和WLN的提取结果图</p> <h1 id="结论">结论</h1> <p>训练标签的错误通常很难被识别和纠正，尤其是在跨时间和地点的遥感数据集中。在本文中，我们提出了一种通用的抗噪声网络框架WLN，基于对每个训练样本进行加权损失的思想，将错误样本对遥感图像分类的影响降到最低。该框架由两个网络组成，即分割子网络和注意力子网络。分段子网络对图像逐个像素进行分类，并计算输出结果与标签，得到训练过程中的初始损失。注意力子网络生成批量样本的权重损失，并与类别平衡系数相结合，防止每个训练样本的类不平衡。这三部分结合得到最终的损失，并对两个子网络进行反推，更新网络参数。</p> <p>通过膨胀和腐蚀处理模拟四种标签噪声(不足标签、冗余标签、缺失标签、错误标签)来测试网络的抗噪声能力。在评估了所提出的WLN与原U-Net模型在Inria航空图像标签数据集中提取建筑物的性能后，我们发现：</p> <p>(1)当噪声率和噪声水平较低时，卷积神经网络几乎不受标签噪声的影响，这可能是由于网络特有的抗噪声能力。当训练集的标签噪声率超过一定阈值后，卷积神经网络的准确率就会明显下降。</p> <p>(2)对于四种标签噪声，如果数据集的样本噪声率和噪声水平逐渐增加，我们提出的方法WLN可以保持较高的精度，并优于原方法。</p> <p>(3)如果我们让网络选择哪些样本是必不可少的，就会发现局部最优问题。这种现象可能具有普遍性，可以通过加入类别平衡系数来调整类标签的不平衡性来缓解。这个问题将在以后的工作中进一步研究。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cmu-steam-tunnels/">The CMU Steam Tunnels and Wean 9</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/HRUNET/">基于HRU-Net的中高分辨率地表要素提取模型</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/HOCPD/">Hocpd</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/IUSTFM/">面向地表温度跨尺度融合的动态神经网络</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/USTFM-LST/">基于解混策略的时空融合模型稳定性分析—以地表温度为例</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 郭善昕 Shanxin Guo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 20, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>