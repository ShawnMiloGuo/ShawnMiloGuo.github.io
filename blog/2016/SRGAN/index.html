<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 基于对抗神经网络ISRGAN的多源遥感数据融合与超分辨模型 | 郭善昕 Shanxin Guo </title> <meta name="author" content="郭善昕 Shanxin Guo"> <meta name="description" content="蒸散空间下的土壤特征空间构建"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shawnmiloguo.github.io/blog/2016/SRGAN/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">郭善昕</span> Shanxin Guo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">主页 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">博客 </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">论文发表 </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">研究课题 </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">研究团队 </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">学术活动 </a> </li> <li class="nav-item "> <a class="nav-link" href="/recruitment/">招生启事 </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div style="display: none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\bsa}{\boldsymbol{a}} \newcommand{\bsb}{\boldsymbol{b}} \newcommand{\bsc}{\boldsymbol{c}} \newcommand{\bsd}{\boldsymbol{d}} \newcommand{\bse}{\boldsymbol{e}} \newcommand{\bsoldf}{\boldsymbol{f}} \newcommand{\bsg}{\boldsymbol{g}} \newcommand{\bsh}{\boldsymbol{h}} \newcommand{\bsi}{\boldsymbol{i}} \newcommand{\bsj}{\boldsymbol{j}} \newcommand{\bsk}{\boldsymbol{k}} \newcommand{\bsell}{\boldsymbol{\ell}} \newcommand{\bsm}{\boldsymbol{m}} \newcommand{\bsn}{\boldsymbol{n}} \newcommand{\bso}{\boldsymbol{o}} \newcommand{\bsp}{\boldsymbol{p}} \newcommand{\bsq}{\boldsymbol{q}} \newcommand{\bsr}{\boldsymbol{r}} \newcommand{\bss}{\boldsymbol{s}} \newcommand{\bst}{\boldsymbol{t}} \newcommand{\bsu}{\boldsymbol{u}} \newcommand{\bsv}{\boldsymbol{v}} \newcommand{\bsw}{\boldsymbol{w}} \newcommand{\bsx}{\boldsymbol{x}} \newcommand{\bsy}{\boldsymbol{y}} \newcommand{\bsz}{\boldsymbol{z}} \newcommand{\bsA}{\boldsymbol{A}} \newcommand{\bsB}{\boldsymbol{B}} \newcommand{\bsC}{\boldsymbol{C}} \newcommand{\bsD}{\boldsymbol{D}} \newcommand{\bsE}{\boldsymbol{E}} \newcommand{\bsF}{\boldsymbol{F}} \newcommand{\bsG}{\boldsymbol{G}} \newcommand{\bsH}{\boldsymbol{H}} \newcommand{\bsI}{\boldsymbol{I}} \newcommand{\bsJ}{\boldsymbol{J}} \newcommand{\bsK}{\boldsymbol{K}} \newcommand{\bsL}{\boldsymbol{L}} \newcommand{\bsM}{\boldsymbol{M}} \newcommand{\bsN}{\boldsymbol{N}} \newcommand{\bsP}{\boldsymbol{P}} \newcommand{\bsQ}{\boldsymbol{Q}} \newcommand{\bsR}{\boldsymbol{R}} \newcommand{\bsS}{\boldsymbol{S}} \newcommand{\bsT}{\boldsymbol{T}} \newcommand{\bsU}{\boldsymbol{U}} \newcommand{\bsV}{\boldsymbol{V}} \newcommand{\bsW}{\boldsymbol{W}} \newcommand{\bsX}{\boldsymbol{X}} \newcommand{\bsY}{\boldsymbol{Y}} \newcommand{\bsZ}{\boldsymbol{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\mathbbm}{\Bbb} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <header class="post-header"> <h1 class="post-title">基于对抗神经网络ISRGAN的多源遥感数据融合与超分辨模型</h1> <p class="post-meta"> Created in February 03, 2016 </p> <p class="post-tags"> <a href="/blog/2016"> <i class="fa-solid fa-calendar fa-sm"></i> 2016 </a>   ·   <a href="/blog/tag/%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E6%9E%84%E5%BB%BA"> <i class="fa-solid fa-hashtag fa-sm"></i> 特征空间构建</a>   ·   <a href="/blog/category/%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E4%BC%98%E5%8C%96"> <i class="fa-solid fa-tag fa-sm"></i> 特征空间优化</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>为满足具体监测任务尤其是应急响应任务对时空分辨率的要求，需要对多时间、空间尺度的遥感数据进行融合。传统数据融合方法鲁棒性较低，模型无法跨区域、跨传感器应用，因此，本研究基于人工智能领域的超分辨率生成对抗网络（以下简称SRGAN），开展多尺度数据融合方法研究，提出ISRGAN影像超分辨模型。</p> <p>ISRGAN模型以超分辨率生成对抗网络SRGAN为基础，为解决SRGAN模型训练不稳定以及在跨区域和跨传感器上的迁移性不足的问题，针对性地修改了SRGAN的损失函数并对其网络结构进行了改进，使模型训练地更加稳定，在跨区域和跨传感器上有着良好的迁移能力。</p> <h1 id="原文链接">原文链接</h1> <p>Xiong, Y., Guo, S., Chen, J., Deng, X., &amp; Sun, L. (2020). Improved SRGAN for Remote Sensing Image Super-Resolution Across Locations and Sensors. Remote Sensing, 12(1263), 1–22. https://doi.org/10.3390/rs12081263</p> <h1 id="研究背景">研究背景</h1> <p>详细和准确的土地覆盖和土地利用空间变化信息是地方生态和环境研究的重要基础。对于这些任务，需要高空间分辨率图像来捕捉地球表面的时空动态变化过程 ^[1]^ 。目前获取高时空分辨率影像的方法主要有两种：1）多源影像融合模型 和2）影像超分辨率模型 。</p> <p>与图像融合模型相比，超分辨模型不需要在相近的时间内获取相同区域的高空间分辨率影像，使得这些方法在计算机视觉和遥感领域的不同场景中都具有更强的可操作性。</p> <p>图像超分辨率模型的基本假设是，如果低空间分辨率图像遵循与创建低空间分辨率图像相同的重采样过程，则可以重建或从其他高空间分辨率图像中学习到低空间分辨率图像中缺失的细节。基于这个假设，在过去的十年里，人们致力于准确地预测点扩散函数，它代表了形成低分辨率像素的混合过程。主要有三种方法：1）基于插值的方法，2）基于重构的方法，3）基于学习的方法（表 1）。</p> <p>首先，插值法是基于一定的数学策略，从相关点计算要恢复的目标点的像素值，其复杂度低，效率高。但是，得到的图像的边缘效果很明显，在插值过程中没有产生新的信息，无法恢复图像的细节。</p> <p>其次，重构法对成像过程进行建模，整合来自同一场景的不同信息，获得高质量的重构结果 。通常这些方法为了提高空间分辨率而牺牲时间分辨率，需要预先配准和大量的算力。</p> <p>第三，学习法克服了难以确定重建方法的分辨率改进倍数的限制，可以面向单一图像，这是目前超分辨率重建的主要发展方向 。在这类方法中，常用的方法有近邻嵌入法 、稀疏表示法 和深度学习法。</p> <p>表 1超分辨方法对比</p> <table> <thead> <tr> <th>方法大类</th> <th>模型</th> <th>基本思想</th> <th>优点</th> <th>缺点</th> </tr> </thead> <tbody> <tr> <td>插值法</td> <td>最近邻插值法，二次卷积法，三次卷积法</td> <td>当前像素值可以用邻近的像素表示</td> <td>复杂度低，效率高</td> <td>图像纹理细节无法预测</td> </tr> <tr> <td>重构法</td> <td>联合地图配准，PSF反卷积，稀疏回归法</td> <td>通过重建技术恢复图像物理性质和特征，使点扩散函数进一步恢复图像细节</td> <td>融合同一场景的不同信息，得到高质量重建结果</td> <td>配准需要耗费大量计算时间</td> </tr> <tr> <td>学习法</td> <td>邻域嵌入法，稀疏表达法，贝叶斯网络，SRCNN，SRGAN</td> <td>通过学习大量图像样本，创建点扩散函数</td> <td>样本数量较多时生成的图像更接近目标图像，获得更高的PSNR</td> <td>训练时间长，需要大量数据集，模型泛化能力差</td> </tr> </tbody> </table> <p>学习法通常需要高度代表性来覆盖整个总体数据变化的训练样本。在实践中，为了达到这一目的，通常需要收集大量的训练样本。但是在遥感领域，几乎不可能准备这样的训练样本集，因为遥感数据的变化不仅取决于目标的变化，还取决于不同的位置和不同的卫星传感器。由于这种限制，许多基于学习的方法都局限于某个位置和特定的传感器，导致模型跨区域和传感器的泛化能力有限。这一限制仍然给为不同区域和不同传感器训练一个超分辨率模型带来了挑战。</p> <p>近年来，随着人工智能特别是基于神经网络的深度学习方法的快速发展，深度学习对于大样本的非线性过程拟合有着明显的优势，在计算机视觉领域得到了广泛的应用。这些模型的一个优点是能够处理大样本集，同时保持良好的泛化能力。在图像超分辨率领域，2014年Dong首次提出了超分辨率的神经网络SRCNN ^[16]^ 。与传统的超分辨率图像相比，该方法取得了更高的峰值信噪比，但当图像上的采样比较高时，重建图像会过于平滑，导致细节丢失。为了克服这一不足，Ledig,<br> C.提出了一种超分辨率生成对抗网络模型SRGAN，将原始CNN结构替换为GAN ^[17]^ 。作为深度学习领域最新的学习模型，SRGAN在捕获大样本的高维非线性特征方面显示出许多优势。然而，SRGAN模型在不同位置和不同传感器的遥感图像上的泛化能力仍然未知。</p> <p>在本研究中，验证了基于GAN的方法可以提高跨区域和传感器的泛化能力，通过一些修改，可以一次性训练，并将结果应用于不同的区域和传感器。本研究主要贡献如下：</p> <ul> <li> <p>1）在SRGAN的基础上，本文提出了ISRGAN，解决了SRGAN训练不稳定以及模型泛化能力弱的问题；</p> </li> <li> <p>2）计算模型在广东GF1数据集和新疆GF1数据集上的定量指标并进行t检验，验证了模型在跨区域上的普适性；</p> </li> <li> <p>3）计算模型在新疆GF1数据集和新疆Landsat8数据集上的定量指标并进行t检验，验证了模型在跨传感器上的普适性；</p> </li> <li> <p>4）以应用服务为目标，以遥感图像超分辨为基础，将超分辨后的遥感图像应用于土地覆盖分类和地物提取，提高分类及地物提取精度。</p> </li> </ul> <h1 id="模型跨区域和跨传感器超分辨结果分析">模型跨区域和跨传感器超分辨结果分析</h1> <p>基于广东高分1号数据集上训练的ISRGAN超分辨模型，我们分别迁移到新疆高分1号数据集、新疆Landsat8数据集上进行模型的跨区域，跨传感器测试。测试部分结果分别如图2.1.2~图2.1.4所示。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320170737-s85vdps.png" alt="image.png" class="rounded"></p> <p>图 2.1.2在广东训练完成的ISRGAN模型在广东本地GF1数据集超分辨率测试结果（a）输入的图像；（b）超分辨图像；（c）地面真值；（d）（e）（f）分别表示超分辨图像真实图像在红、绿和蓝三波段1:1灰度图，斜率分别为1.0063，1.0032和0.9955。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320170817-bmbxu9t.png" alt="image.png" class="rounded"></p> <p>图 2.1.3模型跨区域检验：在广东训练完成的ISRGAN模型直接迁移到新疆GF1数据集超分辨率测试结果（a）输入的图像；（b）超分辨图像；（c）地面真值；（d）（e）（f）分别表示超分辨图像真实图像在红、绿和蓝三波段1:1灰度图，斜率分别为0.9658，0.9378和0.9485。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320170844-0s7hyrz.png" alt="image.png" class="rounded"></p> <p>图2.1.4模型跨传感器检验：将广东训练完成的ISRGAN模型在直接迁移到新疆Landsat数据集的超分辨率测试结果（a）输入的图像；（b）超分辨图像；（c）地面真值；（d）（e）（f）分别表示超分辨图像真实图像在红、绿和蓝三波段1:1灰度图，斜率分别为0.9527，0.9564和0.9760。</p> <h1 id="与传统超分辨模型对比试验">与传统超分辨模型对比试验</h1> <p>本文主要对比了超分辨领域经典方法领域嵌入法及稀疏表达法，根据相应的超分辨结果，计算其与原始图像的定量化指标，在本文中，由于同一时间同一场景下的Landsat8卫星数据和高分一号卫星数据很难获取，而且其对应的像元个数不一致，考虑到后续在验证模型在跨区域和跨传感器上的普适性上计算标准的一致性，故本文的所有定量计算的参考影像均为超分辨之前的原始影像，计算方法为将超分辨后的影像进行相应的降采样处理，再计算其与原始影像的量化指标。图2.1.5展示了本文基于ISRGAN方法和领域嵌入法、稀疏表达法及SRGAN在3个测试集中测试的部分对比结。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320171422-tfhuzi7.png" alt="image.png" class="rounded"></p> <h1 id="结论">结论</h1> <p>本文以生成对抗网络在计算机视觉上的超分辨算法为基础，针对生成对抗网络本身在训练时存在的梯度消失和模式崩坏等问题，结合WGAN中所提出的最小化Wasserstein距离的方法，对原始超分辨网络（SRGAN）进行修改，提出了ISRGAN网络，并将其应用在遥感影像超分辨上，主要得出以下结论：</p> <p>（1）本文所提出的ISRGAN超分辨网络，将其应用在遥感影像超分辨上，所取得的效果在定量指标上要优于领域嵌入法及稀疏表达法等传统影像超分辨方法。</p> <p>（2）为了实现超分辨模型的一次训练，多次使用，我们直接将在用广东高分1影像数据训练的模型应用在新疆高分1影像数据，并对两组数据集超分辨结果的定量指标做t检验，验证了超分辨模型在跨区域上具有普适性。</p> <p>（3）为了结合国产高分1影像数据与Landsat8影像数据各自的优势，本文将在高分1数据上训练的超分辨模型直接应用在Landsat8数据，并对超分辨结果的定量指标做t检验，验证了超分辨模型在跨传感器上具有普适性。</p> <p>（4）以土地利用分类和地物提取为例，对比Landsat8影像超分辨前后的分类和提取精度，其中土地利用分类采用K-means聚类算法，地物提取使用SVM算法。结果表明，超分辨影像在分类和地物提取的目视效果及精度均有明显提高，表明遥感影像超分辨在资源开发、环境监测、灾害研究和全球变化分析等方面极具应用价值。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/IUSTFM/">面向地表快速变化场景的时空融合的动态神经网络I-USTFM：以地表温度为例</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/HRUNET/">基于HRU-Net的中高分辨率地表要素提取模型</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/HOCPD/">Hocpd</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/USTFM-LST/">基于解混策略的时空融合模型稳定性分析—以地表温度为例</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/HQODMRSI-CMD/">使用级联多级检测器的多分辨率遥感图像的高质量目标检测</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 郭善昕 Shanxin Guo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 21, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>