<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 基于HRU-Net的中高分辨率地表要素提取模型 | 郭善昕 Shanxin Guo </title> <meta name="author" content="郭善昕 Shanxin Guo"> <meta name="description" content="蒸散空间下的土壤特征空间构建"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shawnmiloguo.github.io/blog/2016/HRUNET/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">郭善昕</span> Shanxin Guo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">主页 </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">博客 </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">论文 </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">项目 </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">团队 </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">新闻 </a> </li> <li class="nav-item "> <a class="nav-link" href="/recruitment/">招生 </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div style="display: none"> $$ \newcommand{\bone}{\mathbf{1}} \newcommand{\bbeta}{\mathbf{\beta}} \newcommand{\bdelta}{\mathbf{\delta}} \newcommand{\bepsilon}{\mathbf{\epsilon}} \newcommand{\blambda}{\mathbf{\lambda}} \newcommand{\bomega}{\mathbf{\omega}} \newcommand{\bpi}{\mathbf{\pi}} \newcommand{\bphi}{\mathbf{\phi}} \newcommand{\bvphi}{\mathbf{\varphi}} \newcommand{\bpsi}{\mathbf{\psi}} \newcommand{\bsigma}{\mathbf{\sigma}} \newcommand{\btheta}{\mathbf{\theta}} \newcommand{\btau}{\mathbf{\tau}} \newcommand{\ba}{\mathbf{a}} \newcommand{\bb}{\mathbf{b}} \newcommand{\bc}{\mathbf{c}} \newcommand{\bd}{\mathbf{d}} \newcommand{\be}{\mathbf{e}} \newcommand{\boldf}{\mathbf{f}} \newcommand{\bg}{\mathbf{g}} \newcommand{\bh}{\mathbf{h}} \newcommand{\bi}{\mathbf{i}} \newcommand{\bj}{\mathbf{j}} \newcommand{\bk}{\mathbf{k}} \newcommand{\bell}{\mathbf{\ell}} \newcommand{\bm}{\mathbf{m}} \newcommand{\bn}{\mathbf{n}} \newcommand{\bo}{\mathbf{o}} \newcommand{\bp}{\mathbf{p}} \newcommand{\bq}{\mathbf{q}} \newcommand{\br}{\mathbf{r}} \newcommand{\bs}{\mathbf{s}} \newcommand{\bt}{\mathbf{t}} \newcommand{\bu}{\mathbf{u}} \newcommand{\bv}{\mathbf{v}} \newcommand{\bw}{\mathbf{w}} \newcommand{\bx}{\mathbf{x}} \newcommand{\by}{\mathbf{y}} \newcommand{\bz}{\mathbf{z}} \newcommand{\bA}{\mathbf{A}} \newcommand{\bB}{\mathbf{B}} \newcommand{\bC}{\mathbf{C}} \newcommand{\bD}{\mathbf{D}} \newcommand{\bE}{\mathbf{E}} \newcommand{\bF}{\mathbf{F}} \newcommand{\bG}{\mathbf{G}} \newcommand{\bH}{\mathbf{H}} \newcommand{\bI}{\mathbf{I}} \newcommand{\bJ}{\mathbf{J}} \newcommand{\bK}{\mathbf{K}} \newcommand{\bL}{\mathbf{L}} \newcommand{\bM}{\mathbf{M}} \newcommand{\bN}{\mathbf{N}} \newcommand{\bP}{\mathbf{P}} \newcommand{\bQ}{\mathbf{Q}} \newcommand{\bR}{\mathbf{R}} \newcommand{\bS}{\mathbf{S}} \newcommand{\bT}{\mathbf{T}} \newcommand{\bU}{\mathbf{U}} \newcommand{\bV}{\mathbf{V}} \newcommand{\bW}{\mathbf{W}} \newcommand{\bX}{\mathbf{X}} \newcommand{\bY}{\mathbf{Y}} \newcommand{\bZ}{\mathbf{Z}} \newcommand{\bsa}{\boldsymbol{a}} \newcommand{\bsb}{\boldsymbol{b}} \newcommand{\bsc}{\boldsymbol{c}} \newcommand{\bsd}{\boldsymbol{d}} \newcommand{\bse}{\boldsymbol{e}} \newcommand{\bsoldf}{\boldsymbol{f}} \newcommand{\bsg}{\boldsymbol{g}} \newcommand{\bsh}{\boldsymbol{h}} \newcommand{\bsi}{\boldsymbol{i}} \newcommand{\bsj}{\boldsymbol{j}} \newcommand{\bsk}{\boldsymbol{k}} \newcommand{\bsell}{\boldsymbol{\ell}} \newcommand{\bsm}{\boldsymbol{m}} \newcommand{\bsn}{\boldsymbol{n}} \newcommand{\bso}{\boldsymbol{o}} \newcommand{\bsp}{\boldsymbol{p}} \newcommand{\bsq}{\boldsymbol{q}} \newcommand{\bsr}{\boldsymbol{r}} \newcommand{\bss}{\boldsymbol{s}} \newcommand{\bst}{\boldsymbol{t}} \newcommand{\bsu}{\boldsymbol{u}} \newcommand{\bsv}{\boldsymbol{v}} \newcommand{\bsw}{\boldsymbol{w}} \newcommand{\bsx}{\boldsymbol{x}} \newcommand{\bsy}{\boldsymbol{y}} \newcommand{\bsz}{\boldsymbol{z}} \newcommand{\bsA}{\boldsymbol{A}} \newcommand{\bsB}{\boldsymbol{B}} \newcommand{\bsC}{\boldsymbol{C}} \newcommand{\bsD}{\boldsymbol{D}} \newcommand{\bsE}{\boldsymbol{E}} \newcommand{\bsF}{\boldsymbol{F}} \newcommand{\bsG}{\boldsymbol{G}} \newcommand{\bsH}{\boldsymbol{H}} \newcommand{\bsI}{\boldsymbol{I}} \newcommand{\bsJ}{\boldsymbol{J}} \newcommand{\bsK}{\boldsymbol{K}} \newcommand{\bsL}{\boldsymbol{L}} \newcommand{\bsM}{\boldsymbol{M}} \newcommand{\bsN}{\boldsymbol{N}} \newcommand{\bsP}{\boldsymbol{P}} \newcommand{\bsQ}{\boldsymbol{Q}} \newcommand{\bsR}{\boldsymbol{R}} \newcommand{\bsS}{\boldsymbol{S}} \newcommand{\bsT}{\boldsymbol{T}} \newcommand{\bsU}{\boldsymbol{U}} \newcommand{\bsV}{\boldsymbol{V}} \newcommand{\bsW}{\boldsymbol{W}} \newcommand{\bsX}{\boldsymbol{X}} \newcommand{\bsY}{\boldsymbol{Y}} \newcommand{\bsZ}{\boldsymbol{Z}} \newcommand{\calA}{\mathcal{A}} \newcommand{\calB}{\mathcal{B}} \newcommand{\calC}{\mathcal{C}} \newcommand{\calD}{\mathcal{D}} \newcommand{\calE}{\mathcal{E}} \newcommand{\calF}{\mathcal{F}} \newcommand{\calG}{\mathcal{G}} \newcommand{\calH}{\mathcal{H}} \newcommand{\calI}{\mathcal{I}} \newcommand{\calJ}{\mathcal{J}} \newcommand{\calK}{\mathcal{K}} \newcommand{\calL}{\mathcal{L}} \newcommand{\calM}{\mathcal{M}} \newcommand{\calN}{\mathcal{N}} \newcommand{\calO}{\mathcal{O}} \newcommand{\calP}{\mathcal{P}} \newcommand{\calQ}{\mathcal{Q}} \newcommand{\calR}{\mathcal{R}} \newcommand{\calS}{\mathcal{S}} \newcommand{\calT}{\mathcal{T}} \newcommand{\calU}{\mathcal{U}} \newcommand{\calV}{\mathcal{V}} \newcommand{\calW}{\mathcal{W}} \newcommand{\calX}{\mathcal{X}} \newcommand{\calY}{\mathcal{Y}} \newcommand{\calZ}{\mathcal{Z}} \newcommand{\R}{\mathbb{R}} \newcommand{\C}{\mathbb{C}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\F}{\mathbb{F}} \newcommand{\Q}{\mathbb{Q}} \DeclareMathOperator*{\argmax}{arg\,max} \DeclareMathOperator*{\argmin}{arg\,min} \newcommand{\nnz}[1]{\mbox{nnz}(#1)} \newcommand{\dotprod}[2]{\langle #1, #2 \rangle} \newcommand{\ignore}[1]{} \let\Pr\relax \DeclareMathOperator*{\Pr}{\mathbf{Pr}} \newcommand{\E}{\mathbb{E}} \DeclareMathOperator*{\Ex}{\mathbf{E}} \DeclareMathOperator*{\Var}{\mathbf{Var}} \DeclareMathOperator*{\Cov}{\mathbf{Cov}} \DeclareMathOperator*{\stddev}{\mathbf{stddev}} \DeclareMathOperator*{\avg}{avg} \DeclareMathOperator{\poly}{poly} \DeclareMathOperator{\polylog}{polylog} \DeclareMathOperator{\size}{size} \DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\dist}{dist} \DeclareMathOperator{\vol}{vol} \DeclareMathOperator{\spn}{span} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\Tr}{Tr} \DeclareMathOperator{\codim}{codim} \DeclareMathOperator{\diag}{diag} \newcommand{\PTIME}{\mathsf{P}} \newcommand{\LOGSPACE}{\mathsf{L}} \newcommand{\ZPP}{\mathsf{ZPP}} \newcommand{\RP}{\mathsf{RP}} \newcommand{\BPP}{\mathsf{BPP}} \newcommand{\P}{\mathsf{P}} \newcommand{\NP}{\mathsf{NP}} \newcommand{\TC}{\mathsf{TC}} \newcommand{\AC}{\mathsf{AC}} \newcommand{\SC}{\mathsf{SC}} \newcommand{\SZK}{\mathsf{SZK}} \newcommand{\AM}{\mathsf{AM}} \newcommand{\IP}{\mathsf{IP}} \newcommand{\PSPACE}{\mathsf{PSPACE}} \newcommand{\EXP}{\mathsf{EXP}} \newcommand{\MIP}{\mathsf{MIP}} \newcommand{\NEXP}{\mathsf{NEXP}} \newcommand{\BQP}{\mathsf{BQP}} \newcommand{\distP}{\mathsf{dist\textbf{P}}} \newcommand{\distNP}{\mathsf{dist\textbf{NP}}} \newcommand{\eps}{\epsilon} \newcommand{\lam}{\lambda} \newcommand{\dleta}{\delta} \newcommand{\simga}{\sigma} \newcommand{\vphi}{\varphi} \newcommand{\la}{\langle} \newcommand{\ra}{\rangle} \newcommand{\wt}[1]{\widetilde{#1}} \newcommand{\wh}[1]{\widehat{#1}} \newcommand{\ol}[1]{\overline{#1}} \newcommand{\ul}[1]{\underline{#1}} \newcommand{\ot}{\otimes} \newcommand{\zo}{\{0,1\}} \newcommand{\co}{:} %\newcommand{\co}{\colon} \newcommand{\bdry}{\partial} \newcommand{\grad}{\nabla} \newcommand{\transp}{^\intercal} \newcommand{\inv}{^{-1}} \newcommand{\symmdiff}{\triangle} \newcommand{\symdiff}{\symmdiff} \newcommand{\half}{\tfrac{1}{2}} \newcommand{\mathbbm}{\Bbb} \newcommand{\bbone}{\mathbbm 1} \newcommand{\Id}{\bbone} \newcommand{\SAT}{\mathsf{SAT}} \newcommand{\bcalG}{\boldsymbol{\calG}} \newcommand{\calbG}{\bcalG} \newcommand{\bcalX}{\boldsymbol{\calX}} \newcommand{\calbX}{\bcalX} \newcommand{\bcalY}{\boldsymbol{\calY}} \newcommand{\calbY}{\bcalY} \newcommand{\bcalZ}{\boldsymbol{\calZ}} \newcommand{\calbZ}{\bcalZ} $$ </div> <header class="post-header"> <h1 class="post-title">基于HRU-Net的中高分辨率地表要素提取模型</h1> <p class="post-meta"> Created in February 03, 2016 </p> <p class="post-tags"> <a href="/blog/2016"> <i class="fa-solid fa-calendar fa-sm"></i> 2016 </a>   ·   <a href="/blog/tag/%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E6%9E%84%E5%BB%BA"> <i class="fa-solid fa-hashtag fa-sm"></i> 特征空间构建</a>   ·   <a href="/blog/category/%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4%E4%BC%98%E5%8C%96"> <i class="fa-solid fa-tag fa-sm"></i> 特征空间优化</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>多光谱遥感中，由于同物异谱效应，采用传统分类方法（如支持向量机、随机森林）对类似耕地这样的复合要素（休耕、弃耕、轮种情况下的耕地光谱差异较大）提取精度较低。卷积神经网络（CNN）对同一类地物的特征类内差异容忍度较高，具有较强的泛化能力，在同物异谱情况下有望提高复合要素的提取精度。 </p> <p>本文从网络结构和损失函数表达两个方面入手，专注于解决原有深度卷积网络应用到遥感地物分类时存在的高分辨率信息丢失问题。本文提出的方法是在U-Net网络框架基础上，通过结合GridNet，interlinked CNNds ，HRNet等网络全程保持高分辨率信息的核心思想，改进了网络的跳跃连接结构，提出的高分辨HRU-Net网络。于此同时，在HRU-NET设置损失函数中引入深度监督的思想，进一步保留的高分辨率信息训练网络参数。本文以实验以Landsat<br> 4-5 TM传感器为数据源，以新疆卡拉水库附件建设兵团农田为实验区，进行模型验证。该方法可以进一步推广到其他具有同物异谱地物的分类任务中，以提高精度和地物分类细节丰富度。</p> <h1 id="原文链接">原文链接</h1> <p>Xu, W., Deng, X., Guo, S., Chen, J., Sun, L., Zheng, X., Xiong, Y., Shen, Y., &amp; Wang, X. (2020). High-Resolution U-Net : Preserving Image Details for Extraction Cultivated Land. Sensors, 20(15), 4064.</p> <p>(https://www.mdpi.com/1424-8220/20/15/4064/html)</p> <h1 id="研究背景">研究背景</h1> <p>耕地作为重要的土地利用/土地覆盖类型，其数量、质量和空间分布范围关系着人类社会和经济的发展，关乎国家粮食安全问题，且与生态环境保护紧密相连。准确、快速地获取耕地信息是土地利用/土地覆盖研究领域的热点之一。遥感技术为提取耕地类型提供了更加快速、全面、准确的手段，利用遥感图像分类方法提取耕地信息，了解耕地分布、耕地类型及耕地面积等对有效管理作物种植和优化作物种植结构有重要意义。</p> <p>传统的遥感分类方法（SVM, KNN, 和RF等）用于耕地提取时的难点主要有三个方面：1）耕地严重的同物异谱现象：耕地上种植的作物类型多种多样，灌溉方式和土壤类型存在差异，同时存在未覆盖地物的休耕期耕地，导致不同耕地的光谱特征差别明显，而传统的遥感分类方法，以扩大类间差距，减少类内差距为优化目标，不能很好的适应覆盖不同地物的耕地提取要求 ^[33,34]^ 。2）传统遥感分类方法采用的有限的特征，并且这些特征往往针对具体问题进行设计，特征跨地域泛化表征性不强，在研究区内训练的方法，很难在其他区域进行直接应用 ^[35,36]^ 。3）基于统计学习的传统算法，基于样点得到对象在特征空间的分布信息，算法复杂度较高，当训练样本较大时，会出现无法训练或精度饱和等现象，不适合处理大规模的遥感图像数据。</p> <p>近年来，深度学习在遥感图像分类领域发展迅速 ^[37,38]^ ，主要采用的有图片级分类和像素级分类两种方法。1）图片级分类算法，该方法以单个图像为判别单元，每个图像只能包含一种地物类别，通过卷积神经网络对图像的整体特征进行学习。这类算法的核心是图像的识别，通过将整幅影像切割成包含单一地物的若干子影像后，分别对子影像中的地物进行识别。这种方法的优势在于能够较好的利用领域特征，从而提高准确识别地物的类别。但缺点是无法给出像素级的分类结果。因此目前这里方法大多使用在地物识别和提取的应用场景下。例如，Alshehhi等提出了一种卷积神经网络，从高分辨率遥感数据中提取道路和建筑物。Long等提出了一种高分辨率遥感图像的三步物体定位算法，模拟了Fast R-CNN的工作流程，实现地物要素的提取 ^[39]^ 。2）像素级分类算法，以每个像素为判别单元，采用的全卷积网络去掉了卷积神经网络中的全连接层，换成了1*1卷积层，来实现端到端（像素到像素）的分类方法。这种替换保留了图像内容的空间信息，解除了卷积神经网络对输入图像大小的限制，同时大大减少了模型参数量，提高了算法效率。其中具有代表性的有，Jamie Sherrah提出了一种不包括常规下采样层的FCN算法，在ISPRS数据集中实现了89.1%的总体精度 ^[40]^ 。Marmanis等人设计了一个像素级分割架构，合成FCN和反卷积网络，并将CRF应用于后处理以进行细化，在基于ISPRS Vaihingen数据集标签的人工数据集中取得了88.5%的总体精度 ^[41]^ 。Chen等人采用叠加策略对FCN的分段结果进行后处理，相比传统的FCN-8和SegNet模型具有更高的精度。</p> <p>然而，在将像素级分类算法应用到遥感影像耕地提取过程中，为了获取不同尺度区域特征，深度卷积网络往往需要将高分辨率图像转化为低分辨率图像（polling）,来提取抽象不通尺度的语义信息作为特征用于后续分类。而重采样是常用的方式之一，这个过程造成图像高分辨信息（边缘信息，梯度信息或高频噪声信号等）的丢失，使得耕地提取结果边缘模糊，细节不够丰富准确，影响最终的耕地提取精度。</p> <p>目前在全卷积网络用于像素级分类的过程中，解决高分辨率丢失问题的方法大致分为两类：1）从低分辨率表达中学习恢复高分辨率信息2）网络结构中全程保持高分辨率信息。</p> <p>第一类，从低分辨率表达中学习恢复高分辨率信息。这类方法的核心思想是移除了卷积神经网络中的全连接层，从而得到低分辨率特征图，再从低分辨率特征图中学习得到高分辨率信息估计值。例如FCN通过对低分辨率特征图进行双线性插值得到不同尺度的高分辨率特征图，再将高分辨率特征图与网络提取特征过程中得到的相应尺度的特征图进行融合，以期更好的恢复高分辨率信息。另外，采用上采样子网，如解码器，逐步恢复由下采样过程输出的低分辨率特征图的高分辨率表示，也是一种常用的方法。上采样子网可以采用与下采样过程对称的形式，其中，SegNet ^[42]^ 和DeconvNet ^[43]^ 网络通过记录下采样过程中的池化索引，然后通过对应的反池化操作来进行上采样过程 ,逐步恢复图像高分辨率信息，而U-Net ^[44]^ ，FPN网络则增加了跳跃连接过程，将下采样子网与上采样子网中相应分辨率尺度的特征图进行融合操作，进一步恢复高分辨率信息。非对称上采样过程也被广泛使用，一些研究通过采用更复杂的卷积模块来改进跳跃连接过程(C. Peng, X.,2017;Z. Zhang,2018;M. A. Islam，2017)，另外一些研究则通过堆叠多个DeconvNet/UNet/Hourglass 来不断恢复高分率信息 ^[45,46]^ 。这类方法都是通过对低分辨信息进行学习来恢复高分辨率信息，尽管采用了各种跳跃连接方式来优化得到的高分辨率信息，但从低分辨率特征图中获取高分辨率特征的本质是一个病态推算的过程，在遥感地物分类的实际应用中，很难恢复出地物原有的细节纹理。</p> <p>第二类，全程保持高分辨率信息。这类方法在整个网络过程中一直保持高分辨率信息。用来保持高分辨率信息的网络结构一般包括连接多尺度信息（从高分辨率信息到低分辨率信息）的平行结构和融合不同尺度信息的多尺度信息交换结构。代表网络有GridNet，convolutional neural fabrics，interlinked CNNds和高分辨率网络（HRNet）等。其中，较早期的两个方法，convolutional neural fabrics 和interlinked CNNs，对何时开始低分辨率并行流以及如何跨并行流交换信息缺乏谨慎设计，未取得令人满意的结果。GridNet则类似于多个U-Nets的组合，包括两个对称信息交换阶段：第一阶段仅将信息从高分辨率传递到低分辨率，第二阶段仅将信息从低分辨率传递到高分辨率，这也限制了其分割质量。HRNet设计的并行连接结构和重复的不同尺度分辨率信息的交换融合操作则更好地保留了高分辨率信息，取得了更好的结果。然而这部分模型大多应用在自然图像的像素级分类过程中，通道个数与网络深度受到限制，不利于应用到遥感多波段影像的地物分类中。</p> <p>由于卫星遥感图像成像机理和平台的不同，图像中的高分辨率特征有可能是地物的细节信号（边缘信号，纹理信号），也可能是影像的噪声信号（传感器噪声，灰土量化噪声等），如何在有效抑制噪声，尽可能的保留深度卷积网络中图像的高分辨率信号，是将卷积神经网络引入卫星遥感地物分类的重要问题。</p> <p>针对上述问题，本文从网络结构和损失函数表达两个方面入手，专注于解决原有深度卷积网络应用到遥感地物分类时存在的高分辨率信息丢失问题。本文提出的方法是在U-Net网络框架基础上，通过结合GridNet，interlinked CNNds ，HRNet等网络全程保持高分辨率信息的核心思想，改进了网络的跳跃连接结构，提出的高分辨HRU-Net网络。于此同时，在HRU-NET设置损失函数中引入深度监督的思想，进一步保留的高分辨率信息训练网络参数。本文以实验以Landsat 4-5 TM传感器为数据源，以新疆卡拉水库附件建设兵团农田为实验区，进行模型验证。该方法可以进一步推广到其他具有同物异谱地物的分类过程中，以提高精度和地物分类细节丰富度。</p> <h1 id="高分辨率u-nethru-net算法介绍">高分辨率U-Net（HRU-Net）算法介绍</h1> <p>本文提出的HRU-Net方法，保留了U-Net网络的收缩路径和扩张路径，整个网络和U-Net网络一样具有5层分辨率尺度。改进之处主要体现为两点：</p> <p>（1）采用保持高分辨率细节信息的思想，改进U-Net中跳跃连接结构。</p> <p>（2）为了更好的利用各种分辨率特征图的信息，更好的传递梯度信息来学习网络参数，设计损失函数输入时，运用了深度监督的思想。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320173848-7vym0hr.png" alt="image.png" class="rounded"></p> <h1 id="hru-net与u-netu-net-和rf的比较">HRU-Net与U-Net，U-Net ++和RF的比较</h1> <p>本研究从三个方面比较了HRU-Net，U-Net，U-Net ++和RF的结果：（1）总体精度，（2）边缘细节的准确性，（3）类间变化的鲁棒性。</p> <p>表3.1.4和图3.1.7显示了在测试数据集上每种方法的精度评估。在这三个数据集上，HRU-Net在总体精度（Acc），Kappa系数（K）和F1-score（F1）都优于其他三个模型。</p> <p>首先，表3.1.4中的结果表明NIR和SWIR波段可以将总体精度提高1％–4％。与TM-NRG和TM-RGB数据集的结果相比，TM-All数据集的准确性最高。当将NIR添加到RF模型中时，精度提高了3.35%，这可能与模型捕获更高尺度特征（例如可能的非线性波段组合）的能力有关。但是因为深度学习模型在这个方面做得更好，所以在添加新的训练波段时，改进的效果并不明显。其次，HRU-Net在所有三个数据集中均实现了最高的提取精度。特别是在TM-All数据集上，HRU-Net的总体精度达到92.81％，与U-Net<br> ++相比提高了1.07％，与U-Net相比提高了2.98％，与RF相比提高了16％。HRU-Net的最佳kappa系数为0.75-0.81，与U-Net<br> ++相比增加0.01-0.02，与U-Net相比增加0.07-0.09，与RF相比增加0.33-0.50。在F1-Score中也可以发现类似的结果。</p> <p>从表3.1.4中可以看出，NIR波段和SWIR波段可以提供一些有用信息来帮助区分耕地和其他耕地，同时对于RF模型的精度提高更大（RF模型的精度提高了1％–4％）。而对于深度学习模型精度提升仅为0.4％–1％。一个可能的原因是深度学习模型具有更多的学习能力，可以提取出更深层次的特征，例如形状和梯度。另一个原因可能是在类间光谱变化较大的情况下，NIR和SWIR波段虽然可以有效地将植被和非植被像素区分开，但是对于耕地和非耕地却不太有效，因为耕地在不同时期可以有植被覆盖和无植被覆盖。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320174014-9n4bjbe.png" alt="image.png" class="rounded"></p> <p>图3.1.7为这三个模型在TM-All数据集上的混淆矩阵。结果表明，HRU-Net模型的召回率和总体精度都是最高的。与U-Net ++，U-Net和RF相比，HRU-Net中的类型1和类型2错误也保持最低。</p> <p>表3.1.5是HRU-Net在50％，60％和70％训练集下的总体精度。正如本研究预期的那样，训练集越小，准确性将越低，但是即使在50％的训练样本，HRU-Net中的准确性也比其他两个模型下降得慢，性能要好。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320174039-m40d837.png" alt="image.png" class="rounded"></p> <p>表3.1.6是HRU-Net，U-Net ++和U-Net训练期间的时间消耗。RF被排除在外，因为它是由CPU而不是GPU训练的；因此，它无法与其他三种基于GPU的算法相提并论。HRU-Net与原始的U-Net相比，由于通过添加更复杂的跳跃连接而涉及了更多的模型参数使得训练时间增加了约2.6倍。与U-Net ++相比，两个网络在级别相同时，参数数量相似，两者消耗的时间也相似。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320174101-kwokrwd.png" alt="image.png" class="rounded"></p> <p><a href="">图</a>3.1.7 Landsat TM-All数据集的测试数据集上的HRU-Net，U-Net ++，U-Net和RF模型的混淆矩阵。</p> <h1 id="边缘细节的准确性">边缘细节的准确性</h1> <p>如图3.1.8所示，通过目视解译来评估边缘细节的准确性。与U-Net ++和U-Net相比，HRU-Net的结果具有更清晰的边缘和更丰富的细节。具体来说，与U-Net<br> ++相比，输出中保留了更详细的边界信息，同时HRU-Net的边缘比原始U-Net的边缘准确得多，而RF的输出图中，边缘不准确，并且没有农作物覆盖的农田由于类间变化RF无法正确检测。</p> <p>图3.1.9显示不同模型的类内变化的鲁棒性。在图3.1.8中，绘制了测试数据集中每个图块的总体精度。如图3.1.9（a）所示，RF模型变化最高，因为其拟合不同谱段的泛化能力最差。图3.1.9（b）显示了HRU-Net，U-Net++，以及U-Net的变化情况，在图3.1.9（b）中，HRU-Net的变化与U-Net<br> ++相似，但是，它在所有三个数据集中总体精度都是最高的。这表明HRU-Net在解决类内变化问题的有效性。</p> <p><img src="/SIAT-GeoScience/assets/image-20220320174159-vph0nlp.png" alt="image.png" class="rounded"></p> <p>图3.1.8 HRU-Net，U-Net和随机森林模型在三个数据集的输出图对比</p> <p><img src="/SIAT-GeoScience/assets/image-20220320174235-dyug82k.png" alt="image.png" class="rounded"></p> <p><img src="/SIAT-GeoScience/assets/image-20220320174256-878bcuw.png" alt="image.png" class="rounded"></p> <p>图3.1.9测试数据集上的总体精度分布的箱线图（868个图块）。(a)RF和深度学习算法之间的比较；(b)HRU-Net，U-Net ++和U-Net之间的比较。</p> <h1 id="结论">结论</h1> <p>本文提出的HRU-Net网络，主要是为了解决两个问题：</p> <p>（1）传统方法进行耕地提取时的同物异谱问题，这使得耕地提取时，未覆盖植被的休耕期耕地很难被提取，提取精度很低。</p> <p>（2）利用深度学习进行耕地提取时高分辨率信息丢失的问题，这不仅导致了耕地提取结果边缘模糊，细节丢失，也使得深度学习网络对于遥感图像的光谱、纹理等信息利用不充分，无法发挥遥感影像多波段、信息丰富的优势，影响最终的耕地提取精度。</p> <p>针对以上两个问题，HR-UNet在全卷积网络U-Net的基础上，保留了U-Net网络的对称编解码结构，进行了以下两方面的改进：</p> <p>（1）根据在全卷积网络中全程保持高分辨率信息的思想，改进了U-Net网络的跳跃连接结构。</p> <p>（2）为了更好的利用保留的高分辨率信息训练网络参数，在设置损失函数时采用了深度监督的思想。</p> <p>在由Landsat影像不同波段数据组成的三个数据集TMall，TMnrg和TMrgb中，使用HRU-Net，U-Net，UNet++ ^[47]^ 和Random Forest分别进行耕地提取实验后，验证了本文提出的HRU-Net网络基本达到预期目标，并能得出以下结论：</p> <p>（1）在耕地提取精度上，相比基于深度学习的U-Net,<br> UNet++网络和传统方法Random Forest，在整体精度，混淆矩阵，kappa系数和F1-score四个评价指标中，均取得了更好的结果。</p> <p>（2）在三个数据集中，三个方法均在包含Landsat影像全6个波段的TMall数据集中取得了最好的结果，其中，HRU-Net方法表现最好，整体耕地精度达到90.61%，Kappa系数达到0.8。</p> <p>（3）在三个数据集中，传统Random<br> Forest方法均未能准确识别出未覆盖地物的休耕期耕地，而HRU-Net方法能准确识别出所有类型的耕地，解决了耕地提取中的同物异谱问题。</p> <p>（4）在三个数据集中，HRU-Net模型相比U-Net模型的耕地结果图边缘更为清晰，细节更为丰富，与遥感影像中耕地的实际分布情况以及标签更为吻合，说明HRU-Net针对高分辨率信息丢失问题的改进取得了明显的效果。</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/cmu-steam-tunnels/">The CMU Steam Tunnels and Wean 9</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/HRUNET/">基于HRU-Net的中高分辨率地表要素提取模型</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2018/HOCPD/">Hocpd</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/IUSTFM/">面向地表温度跨尺度融合的动态神经网络</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/USTFM-LST/">基于解混策略的时空融合模型稳定性分析—以地表温度为例</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 郭善昕 Shanxin Guo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 21, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>